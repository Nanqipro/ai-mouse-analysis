from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from typing import List, Optional
import os
import shutil
import tempfile
import pandas as pd
from datetime import datetime
import json
from pathlib import Path

# å¯¼å…¥æ ¸å¿ƒé€»è¾‘æ¨¡å—
from src.extraction_logic import run_batch_extraction, extract_calcium_features, get_interactive_data, extract_manual_range
from src.clustering_logic import (
    load_data,
    enhance_preprocess_data,
    cluster_kmeans,
    visualize_clusters_2d,
    visualize_feature_distribution,
    analyze_clusters,
    generate_comprehensive_cluster_analysis,
    determine_optimal_k,
    cluster_dbscan,
    create_k_comparison_plot,
    plot_to_base64
)
from src.heatmap_behavior import (
    BehaviorHeatmapConfig,
    load_and_validate_data,
    find_behavior_pairs,
    extract_behavior_sequence_data,
    standardize_neural_data,
    create_behavior_sequence_heatmap,
    create_average_sequence_heatmap,
    get_global_neuron_order
)
from src.overall_heatmap import (
    OverallHeatmapConfig,
    generate_overall_heatmap
)
from src.heatmap_em_sort import (
    EMSortHeatmapConfig,
    analyze_em_sort_heatmap
)
from src.heatmap_multi_day import (
    MultiDayHeatmapConfig,
    analyze_multiday_heatmap
)
import numpy as np
from src.utils import save_plot_as_base64
import base64
from io import BytesIO

app = FastAPI(title="é’™ä¿¡å·åˆ†æå¹³å° API", version="1.0.0")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174"],  # Vueå¼€å‘æœåŠ¡å™¨åœ°å€
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å¢åŠ è¯·æ±‚å¤§å°é™åˆ¶ï¼Œè§£å†³431é”™è¯¯
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

class LimitUploadSizeMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, max_upload_size: int = 200 * 1024 * 1024):  # 200MB
        super().__init__(app)
        self.max_upload_size = max_upload_size

    async def dispatch(self, request: Request, call_next):
        # å¤„ç†è¯·æ±‚å¤´å¤§å°é—®é¢˜
        try:
            if request.method == "POST":
                content_length = request.headers.get("content-length")
                if content_length and int(content_length) > self.max_upload_size:
                    return Response(
                        json.dumps({"detail": f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§å…è®¸ {self.max_upload_size // (1024 * 1024)}MB"}),
                        status_code=413,
                        headers={"content-type": "application/json"}
                    )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤§çš„è¯·æ±‚å¤´
            total_header_size = sum(len(k) + len(v) for k, v in request.headers.items())
            if total_header_size > 32768:  # 32KBé™åˆ¶
                return Response(
                    json.dumps({"detail": "è¯·æ±‚å¤´è¿‡å¤§ï¼Œè¯·å‡å°‘æ–‡ä»¶å¤§å°æˆ–åˆ†æ‰¹ä¸Šä¼ "}),
                    status_code=431,
                    headers={"content-type": "application/json"}
                )
                
            return await call_next(request)
            
        except Exception as e:
            print(f"ä¸­é—´ä»¶é”™è¯¯: {e}")
            return Response(
                json.dumps({"detail": f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"}),
                status_code=500,
                headers={"content-type": "application/json"}
            )

app.add_middleware(LimitUploadSizeMiddleware)

# åˆ›å»ºå¿…è¦çš„ç›®å½•
UPLOADS_DIR = Path("uploads")
RESULTS_DIR = Path("results")
TEMP_DIR = Path("temp")

for dir_path in [UPLOADS_DIR, RESULTS_DIR, TEMP_DIR]:
    dir_path.mkdir(exist_ok=True)

@app.get("/")
async def root():
    return {"message": "é’™ä¿¡å·åˆ†æå¹³å° API"}

@app.post("/api/extraction/preview")
async def preview_extraction(
    file: UploadFile = File(...),
    fs: float = Form(4.8),
    min_duration_frames: int = Form(12),
    max_duration_frames: int = Form(800),
    min_snr: float = Form(3.5),
    smooth_window: int = Form(31),
    peak_distance_frames: int = Form(24),
    filter_strength: float = Form(1.0),
    neuron_id: str = Form(...)
):
    """é¢„è§ˆå•ä¸ªç¥ç»å…ƒçš„äº‹ä»¶æå–ç»“æœ"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # è¯»å–æ•°æ® - é€‚é…element_extraction.pyæ ¼å¼ï¼ˆç›´æ¥è¯»å–Excelæ–‡ä»¶ï¼‰
        df = pd.read_excel(temp_file)
        # æ¸…ç†åˆ—åï¼ˆå»é™¤å¯èƒ½çš„ç©ºæ ¼ï¼‰
        df.columns = [col.strip() for col in df.columns]
        
        # æå–ç¥ç»å…ƒåˆ—ï¼ˆä»¥'n'å¼€å¤´ä¸”åé¢è·Ÿæ•°å­—çš„åˆ—ï¼‰
        neuron_columns = [col for col in df.columns if col.startswith('n') and col[1:].isdigit()]
        
        # å¦‚æœneuron_idæ˜¯'temp'ï¼Œåªè¿”å›ç¥ç»å…ƒåˆ—è¡¨
        if neuron_id == 'temp':
            temp_file.unlink()
            return {
                "success": True,
                "neuron_columns": neuron_columns,
                "features": [],
                "plot": None
            }
        
        if neuron_id not in df.columns:
            raise HTTPException(status_code=400, detail=f"ç¥ç»å…ƒ {neuron_id} ä¸å­˜åœ¨")
        
        # è®¾ç½®å‚æ•°
        params = {
            'min_duration': min_duration_frames,
            'max_duration': max_duration_frames,
            'min_snr': min_snr,
            'smooth_window': smooth_window,
            'peak_distance': peak_distance_frames,
            'filter_strength': filter_strength
        }
        
        # æå–ç‰¹å¾å¹¶ç”Ÿæˆå¯è§†åŒ–
        feature_table, fig, _ = extract_calcium_features(
            df[neuron_id].values, fs=fs, visualize=True, params=params
        )
        
        # å°†å›¾è¡¨è½¬æ¢ä¸ºbase64
        plot_base64 = save_plot_as_base64(fig)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()
        
        return {
            "success": True,
            "features": feature_table.to_dict('records') if not feature_table.empty else [],
            "plot": plot_base64,
            "neuron_columns": neuron_columns
        }
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file.exists():
            temp_file.unlink()
        print(f"Error in preview_extraction: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/extraction/interactive_data")
async def get_interactive_extraction_data(
    file: UploadFile = File(...),
    neuron_id: str = Form(...)
):
    """è·å–äº¤äº’å¼å›¾è¡¨æ•°æ®"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # è·å–äº¤äº’å¼æ•°æ®
        interactive_data = get_interactive_data(str(temp_file), neuron_id)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()
        
        return {
            "success": True,
            "data": interactive_data
        }
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file.exists():
            temp_file.unlink()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/extraction/manual_extract")
async def manual_extraction(
    file: UploadFile = File(...),
    neuron_id: str = Form(...),
    start_time: float = Form(...),
    end_time: float = Form(...),
    fs: float = Form(4.8),
    min_duration_frames: int = Form(5),
    max_duration_frames: int = Form(100),
    min_snr: float = Form(2.0),
    smooth_window: int = Form(5),
    peak_distance_frames: int = Form(10),
    filter_strength: float = Form(0.1)
):
    """åŸºäºç”¨æˆ·é€‰æ‹©çš„æ—¶é—´èŒƒå›´è¿›è¡Œæ‰‹åŠ¨æå–"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"temp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # æ„å»ºå‚æ•°å­—å…¸
        params = {
            'fs': fs,
            'min_duration_frames': min_duration_frames,
            'max_duration_frames': max_duration_frames,
            'min_snr': min_snr,
            'smooth_window': smooth_window,
            'peak_distance_frames': peak_distance_frames,
            'filter_strength': filter_strength
        }
        
        # æ‰§è¡Œæ‰‹åŠ¨æå–
        result = extract_manual_range(str(temp_file), neuron_id, start_time, end_time, params)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()
        
        return result
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file.exists():
            temp_file.unlink()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/extraction/batch")
async def batch_extraction(
    files: List[UploadFile] = File(...),
    fs: float = Form(4.8),
    min_duration_frames: int = Form(12),
    max_duration_frames: int = Form(800),
    min_snr: float = Form(3.5),
    smooth_window: int = Form(31),
    peak_distance_frames: int = Form(24),
    filter_strength: float = Form(1.0)
):
    """æ‰¹é‡å¤„ç†æ–‡ä»¶è¿›è¡Œäº‹ä»¶æå–"""
    try:
        # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        upload_dir = UPLOADS_DIR / timestamp
        upload_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        saved_file_paths = []
        for file in files:
            file_path = upload_dir / file.filename
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            saved_file_paths.append(str(file_path))
        
        # è®¾ç½®å‚æ•°
        params = {
            'min_duration': min_duration_frames,
            'max_duration': max_duration_frames,
            'min_snr': min_snr,
            'smooth_window': smooth_window,
            'peak_distance': peak_distance_frames,
            'filter_strength': filter_strength
        }
        
        # æ‰§è¡Œæ‰¹é‡æå–
        result_path = run_batch_extraction(saved_file_paths, str(RESULTS_DIR), fs=fs, **params)
        
        if result_path and os.path.exists(result_path):
            return {
                "success": True,
                "result_file": os.path.basename(result_path),
                "message": "æ‰¹é‡åˆ†æå®Œæˆ"
            }
        else:
            raise HTTPException(status_code=500, detail="æ‰¹é‡åˆ†ææœªç”Ÿæˆä»»ä½•ç»“æœ")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/extraction/save_preview")
async def save_preview_result(
    data: str = Form(...)
):
    """ä¿å­˜å•ç¥ç»å…ƒé¢„è§ˆç»“æœ"""
    try:
        # è§£æå‰ç«¯ä¼ æ¥çš„æ•°æ®
        save_data = json.loads(data)
        
        # æ„å»ºè¾“å‡ºæ–‡ä»¶å
        original_filename = save_data['filename']
        neuron = save_data['neuron']
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_name = Path(original_filename).stem
        output_filename = f"{base_name}_{neuron}_features_{timestamp}.xlsx"
        output_path = RESULTS_DIR / output_filename
        
        # æ„å»ºDataFrame
        features_data = []
        for i, feature in enumerate(save_data['features']):
            feature_row = {
                'event_id': i + 1,
                'neuron': neuron,
                'amplitude': feature.get('amplitude', 0),
                'duration': feature.get('duration', 0),
                'fwhm': feature.get('fwhm', 0),
                'rise_time': feature.get('rise_time', 0),
                'decay_time': feature.get('decay_time', 0),
                'auc': feature.get('auc', 0),
                'snr': feature.get('snr', 0),
                'start_idx': feature.get('start_idx', 0),
                'peak_idx': feature.get('peak_idx', 0),
                'end_idx': feature.get('end_idx', 0),
                'start_time': feature.get('start_time', 0),
                'peak_time': feature.get('peak_time', 0),
                'end_time': feature.get('end_time', 0),
                'extraction_method': 'manual' if feature.get('isManualExtracted', False) else 'auto',
                'source_file': original_filename
            }
            features_data.append(feature_row)
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(features_data)
        df.to_excel(output_path, index=False)
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = {
            'original_file': original_filename,
            'neuron': neuron,
            'total_features': save_data['total_features'],
            'manual_features': save_data['manual_features'],
            'auto_features': save_data['auto_features'],
            'extraction_params': save_data['params'],
            'created_at': datetime.now().isoformat(),
            'file_type': 'single_neuron_preview'
        }
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = RESULTS_DIR / f"{base_name}_{neuron}_metadata_{timestamp}.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True, 
            "filename": output_filename,
            "features_count": len(features_data),
            "message": f"æˆåŠŸä¿å­˜ {len(features_data)} ä¸ªç‰¹å¾"
        }
        
    except Exception as e:
        print(f"ä¿å­˜é¢„è§ˆç»“æœé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜å¤±è´¥: {str(e)}")

@app.get("/api/results/files")
async def list_result_files():
    """è·å–ç»“æœæ–‡ä»¶åˆ—è¡¨"""
    try:
        feature_files = list(RESULTS_DIR.glob("*_features.xlsx"))
        files_info = []
        
        for file_path in feature_files:
            try:
                # å°è¯•ä»æ–‡ä»¶åè§£ææ—¶é—´æˆ³
                basename = file_path.name
                timestamp_str = basename.split('_features.xlsx')[0].split('_')[-1]
                dt_obj = datetime.strptime(timestamp_str, '%Y%m%d-%H%M%S')
                friendly_name = f"{basename} (åˆ›å»ºäº: {dt_obj.strftime('%Y-%m-%d %H:%M:%S')})"
            except (ValueError, IndexError):
                friendly_name = basename
            
            # è·å–æ–‡ä»¶çš„åˆ›å»ºæ—¶é—´
            stat = file_path.stat()
            created_at = datetime.fromtimestamp(stat.st_mtime).isoformat()
            
            files_info.append({
                "filename": basename,
                "friendly_name": friendly_name,
                "path": str(file_path),
                "created_at": created_at,
                "size": stat.st_size
            })
        
        return {"success": True, "files": files_info}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/clustering/analyze")
async def clustering_analysis(
    filename: str = Form(...),
    k: Optional[int] = Form(None),
    algorithm: str = Form("kmeans"),
    reduction_method: str = Form("pca"),
    feature_weights: Optional[str] = Form(None),
    auto_k: bool = Form(False),
    auto_k_range: str = Form("2,10"),
    dbscan_eps: float = Form(0.5),
    dbscan_min_samples: int = Form(5)
):
    """
    æ‰§è¡Œç»¼åˆèšç±»åˆ†æ
    
    å‚æ•°:
    - filename: æ•°æ®æ–‡ä»¶å
    - k: K-meansèšç±»æ•°ï¼ˆå¦‚æœä¸ºNoneä¸”auto_k=Trueï¼Œåˆ™è‡ªåŠ¨ç¡®å®šï¼‰
    - algorithm: èšç±»ç®—æ³• ('kmeans' æˆ– 'dbscan')
    - reduction_method: é™ç»´æ–¹æ³• ('pca' æˆ– 'tsne')
    - feature_weights: JSONæ ¼å¼çš„ç‰¹å¾æƒé‡å­—å…¸
    - auto_k: æ˜¯å¦è‡ªåŠ¨ç¡®å®šæœ€ä½³Kå€¼
    - auto_k_range: è‡ªåŠ¨ç¡®å®šKå€¼çš„æœç´¢èŒƒå›´ï¼Œæ ¼å¼ "min,max"
    - dbscan_eps: DBSCANçš„epså‚æ•°
    - dbscan_min_samples: DBSCANçš„min_sampleså‚æ•°
    """
    try:
        file_path = RESULTS_DIR / filename
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è§£æç‰¹å¾æƒé‡
        weights = None
        if feature_weights:
            try:
                weights = json.loads(feature_weights)
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="ç‰¹å¾æƒé‡æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºJSONæ ¼å¼")
        
        # è§£æè‡ªåŠ¨Kå€¼èŒƒå›´
        auto_k_min, auto_k_max = map(int, auto_k_range.split(','))
        
        # å¦‚æœå¯ç”¨è‡ªåŠ¨Kå€¼ä¸”ä½¿ç”¨K-meansï¼Œåˆ™å°†kè®¾ä¸ºNone
        if auto_k and algorithm == 'kmeans':
            k = None
        
        # æ‰§è¡Œç»¼åˆèšç±»åˆ†æ
        result = generate_comprehensive_cluster_analysis(
            file_path=str(file_path),
            k=k,
            algorithm=algorithm,
            feature_weights=weights,
            reduction_method=reduction_method,
            auto_k_range=(auto_k_min, auto_k_max),
            dbscan_eps=dbscan_eps,
            dbscan_min_samples=dbscan_min_samples
        )
        
        # æ·»åŠ æˆåŠŸæ ‡å¿—å’Œè¯·æ±‚å‚æ•°
        result.update({
            "success": True,
            "request_params": {
                "filename": filename,
                "k": k,
                "algorithm": algorithm,
                "reduction_method": reduction_method,
                "feature_weights": weights,
                "auto_k": auto_k,
                "auto_k_range": (auto_k_min, auto_k_max)
            }
        })
        
        return result
        
    except Exception as e:
        print(f"èšç±»åˆ†æé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"èšç±»åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/clustering/optimal_k")
async def find_optimal_k(
    filename: str = Form(...),
    max_k: int = Form(10),
    feature_weights: Optional[str] = Form(None)
):
    """
    ç¡®å®šæœ€ä½³èšç±»æ•°K
    """
    try:
        file_path = RESULTS_DIR / filename
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è§£æç‰¹å¾æƒé‡
        weights = None
        if feature_weights:
            try:
                weights = json.loads(feature_weights)
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="ç‰¹å¾æƒé‡æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºJSONæ ¼å¼")
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        df = load_data(str(file_path))
        features_scaled, feature_names, df_clean = enhance_preprocess_data(df, weights)
        
        # ç¡®å®šæœ€ä½³Kå€¼
        optimal_k, inertia_values, silhouette_scores = determine_optimal_k(features_scaled, max_k)
        
        # ç”ŸæˆKå€¼æ¯”è¾ƒå›¾
        from src.clustering_logic import create_optimal_k_plot
        k_range = list(range(2, max_k + 1))
        k_plot = create_optimal_k_plot(inertia_values, silhouette_scores, k_range)
        k_plot_base64 = plot_to_base64(k_plot)
        
        return {
            "success": True,
            "optimal_k": optimal_k,
            "k_range": k_range,
            "inertia_values": inertia_values,
            "silhouette_scores": silhouette_scores,
            "optimal_k_plot": k_plot_base64,
            "data_info": {
                "total_samples": len(df),
                "valid_samples": len(df_clean),
                "features_used": feature_names
            }
        }
        
    except Exception as e:
        print(f"æœ€ä½³Kå€¼åˆ†æé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœ€ä½³Kå€¼åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/clustering/compare_k")
async def compare_k_values(
    filename: str = Form(...),
    k_values: str = Form("2,3,4,5"),
    feature_weights: Optional[str] = Form(None)
):
    """
    æ¯”è¾ƒä¸åŒKå€¼çš„èšç±»æ•ˆæœ
    """
    try:
        file_path = RESULTS_DIR / filename
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è§£æKå€¼åˆ—è¡¨
        k_list = [int(k.strip()) for k in k_values.split(',')]
        
        # è§£æç‰¹å¾æƒé‡
        weights = None
        if feature_weights:
            try:
                weights = json.loads(feature_weights)
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="ç‰¹å¾æƒé‡æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºJSONæ ¼å¼")
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        df = load_data(str(file_path))
        features_scaled, feature_names, df_clean = enhance_preprocess_data(df, weights)
        
        # åˆ›å»ºKå€¼æ¯”è¾ƒå›¾
        comparison_plot, silhouette_scores_dict = create_k_comparison_plot(features_scaled, k_list)
        comparison_plot_base64 = plot_to_base64(comparison_plot)
        
        # æ‰¾å‡ºæœ€ä½³Kå€¼
        best_k = max(silhouette_scores_dict, key=silhouette_scores_dict.get)
        
        return {
            "success": True,
            "k_values": k_list,
            "silhouette_scores": silhouette_scores_dict,
            "best_k": best_k,
            "comparison_plot": comparison_plot_base64,
            "data_info": {
                "total_samples": len(df),
                "valid_samples": len(df_clean),
                "features_used": feature_names
            }
        }
        
    except Exception as e:
        print(f"Kå€¼æ¯”è¾ƒé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Kå€¼æ¯”è¾ƒå¤±è´¥: {str(e)}")

@app.post("/api/behavior/detect")
async def detect_behavior_events(
    file: UploadFile = File(...)
):
    """æ£€æµ‹è¡Œä¸ºäº‹ä»¶é…å¯¹"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"behavior_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"æ£€æµ‹è¡Œä¸ºäº‹ä»¶ï¼Œæ–‡ä»¶: {temp_file}")
        
        # åŠ è½½æ•°æ®
        data = load_and_validate_data(str(temp_file))
        print(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„è¡Œä¸ºç±»å‹
        unique_behaviors = data['behavior'].unique().tolist()
        print(f"å‘ç°çš„è¡Œä¸ºç±»å‹: {unique_behaviors}")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„è¡Œä¸ºé…å¯¹ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
        behavior_events = []
        
        # éå†æ‰€æœ‰å¯èƒ½çš„è¡Œä¸ºé…å¯¹ç»„åˆ
        for start_behavior in unique_behaviors:
            for end_behavior in unique_behaviors:
                if start_behavior != end_behavior:
                    try:
                        pairs = find_behavior_pairs(
                            data, start_behavior, end_behavior, 
                            min_duration=1.0, sampling_rate=4.8
                        )
                        
                        for i, (start_begin, start_end, end_begin, end_end) in enumerate(pairs):
                            behavior_events.append({
                                'index': len(behavior_events) + 1,
                                'start_behavior': start_behavior,
                                'end_behavior': end_behavior,
                                'start_time': float(start_begin / 4.8),  # è½¬æ¢ä¸ºç§’
                                'end_time': float(end_end / 4.8),  # è½¬æ¢ä¸ºç§’
                                'duration': float((end_end - start_begin) / 4.8)  # è½¬æ¢ä¸ºç§’
                            })
                    except Exception as e:
                        print(f"æŸ¥æ‰¾è¡Œä¸ºé…å¯¹ {start_behavior} -> {end_behavior} æ—¶å‡ºé”™: {e}")
                        continue
        
        print(f"æ£€æµ‹åˆ° {len(behavior_events)} ä¸ªè¡Œä¸ºäº‹ä»¶é…å¯¹")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file.exists():
            temp_file.unlink()
        
        return {
            "success": True,
            "behavior_events": behavior_events,
            "available_behaviors": unique_behaviors,
            "message": f"æ£€æµ‹åˆ° {len(behavior_events)} ä¸ªè¡Œä¸ºäº‹ä»¶é…å¯¹"
        }
        
    except Exception as e:
        print(f"è¡Œä¸ºäº‹ä»¶æ£€æµ‹é”™è¯¯: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_file' in locals() and temp_file.exists():
            temp_file.unlink()
        raise HTTPException(status_code=500, detail=f"è¡Œä¸ºäº‹ä»¶æ£€æµ‹å¤±è´¥: {str(e)}")

@app.post("/api/heatmap/analyze")
async def heatmap_analysis(
    file: UploadFile = File(...),
    start_behavior: str = Form(...),
    end_behavior: str = Form(...),
    pre_behavior_time: float = Form(10.0),
    min_duration: float = Form(1.0),
    sampling_rate: float = Form(4.8)
):
    """çƒ­åŠ›å›¾åˆ†æ"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"heatmap_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = BehaviorHeatmapConfig()
        config.INPUT_FILE = str(temp_file)
        config.START_BEHAVIOR = start_behavior
        config.END_BEHAVIOR = end_behavior
        config.PRE_BEHAVIOR_TIME = pre_behavior_time
        config.MIN_BEHAVIOR_DURATION = min_duration
        config.SAMPLING_RATE = sampling_rate
        config.OUTPUT_DIR = str(RESULTS_DIR / "heatmaps")
        config.SORTING_METHOD = 'first'
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)
        
        # åŠ è½½æ•°æ®
        data = load_and_validate_data(str(temp_file))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„è¡Œä¸ºæ•°æ®
        if 'behavior' in data.columns:
            unique_behaviors = data['behavior'].unique()
            if len(unique_behaviors) == 1 and unique_behaviors[0] == 'Unknown':
                raise HTTPException(
                    status_code=400, 
                    detail=f"æ•°æ®æ–‡ä»¶ç¼ºå°‘è¡Œä¸ºæ ‡ç­¾ä¿¡æ¯ã€‚å½“å‰æ–‡ä»¶åªåŒ…å«ç¥ç»å…ƒæ´»åŠ¨æ•°æ®ï¼Œæ— æ³•è¿›è¡Œè¡Œä¸ºçƒ­åŠ›åˆ†æã€‚è¯·ä¸Šä¼ åŒ…å«è¡Œä¸ºæ ‡ç­¾çš„æ•°æ®æ–‡ä»¶ã€‚"
                )
        
        # æŸ¥æ‰¾è¡Œä¸ºé…å¯¹
        behavior_pairs = find_behavior_pairs(
            data, start_behavior, end_behavior, 
            min_duration, sampling_rate
        )
        
        if not behavior_pairs:
            available_behaviors = data['behavior'].unique() if 'behavior' in data.columns else []
            raise HTTPException(
                status_code=400, 
                detail=f"æœªæ‰¾åˆ°ä»'{start_behavior}'åˆ°'{end_behavior}'çš„è¡Œä¸ºé…å¯¹ã€‚æ•°æ®ä¸­å¯ç”¨çš„è¡Œä¸ºç±»å‹: {list(available_behaviors)}"
            )
        
        # æå–æ‰€æœ‰è¡Œä¸ºåºåˆ—æ•°æ®
        all_sequence_data = []
        heatmap_images = []
        first_heatmap_order = None
        valid_pairs_count = 0
        
        for i, (start_begin, start_end, end_begin, end_end) in enumerate(behavior_pairs):
            # æå–è¡Œä¸ºåºåˆ—æ•°æ®
            sequence_data = extract_behavior_sequence_data(
                data, start_begin, end_end, pre_behavior_time, sampling_rate
            )
            
            if sequence_data is not None:
                # æ ‡å‡†åŒ–æ•°æ®
                standardized_data = standardize_neural_data(sequence_data)
                all_sequence_data.append(standardized_data)
                valid_pairs_count += 1
                
                # åˆ›å»ºçƒ­åŠ›å›¾
                fig, current_order = create_behavior_sequence_heatmap(
                    standardized_data, start_begin, end_end,
                    start_behavior, end_behavior, pre_behavior_time,
                    config, i, first_heatmap_order=first_heatmap_order
                )
                
                # ä¿å­˜ç¬¬ä¸€ä¸ªçƒ­åŠ›å›¾çš„æ’åºé¡ºåº
                if valid_pairs_count == 1 and current_order is not None:
                    first_heatmap_order = current_order
                
                # å°†å›¾è¡¨è½¬æ¢ä¸ºbase64
                plot_base64 = save_plot_as_base64(fig)
                heatmap_images.append({
                    "title": f"è¡Œä¸ºé…å¯¹ {valid_pairs_count} çƒ­åŠ›å›¾",
                    "url": f"data:image/png;base64,{plot_base64}"
                })
            else:
                print(f"è·³è¿‡è¡Œä¸ºé…å¯¹ {i+1}: æ—¶é—´èŒƒå›´è¶…å‡ºæ•°æ®èŒƒå›´")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„åºåˆ—æ•°æ®
        if not all_sequence_data:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ³•æå–æœ‰æ•ˆçš„è¡Œä¸ºåºåˆ—æ•°æ®ã€‚æ‰€æœ‰æ‰¾åˆ°çš„è¡Œä¸ºé…å¯¹çš„æ—¶é—´èŒƒå›´éƒ½è¶…å‡ºäº†æ•°æ®èŒƒå›´ã€‚è¯·æ£€æŸ¥è¡Œä¸ºæ—¶é—´å’Œé¢„è¡Œä¸ºæ—¶é—´è®¾ç½®ã€‚"
            )
        
        # åˆ›å»ºå¹³å‡çƒ­åŠ›å›¾
        if len(all_sequence_data) > 1:
            avg_fig = create_average_sequence_heatmap(
                all_sequence_data, start_behavior, end_behavior,
                pre_behavior_time, config, first_heatmap_order=first_heatmap_order
            )
            avg_plot_base64 = save_plot_as_base64(avg_fig)
            heatmap_images.append({
                "title": "å¹³å‡çƒ­åŠ›å›¾",
                "url": f"data:image/png;base64,{avg_plot_base64}"
            })
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()
        
        # è·å–ç¥ç»å…ƒæ•°é‡
        neuron_columns = [col for col in data.columns if col not in ['behavior']]
        
        return {
            "success": True,
            "filename": file.filename,
            "behavior_pairs_count": len(behavior_pairs),
            "neuron_count": len(neuron_columns),
            "start_behavior": start_behavior,
            "end_behavior": end_behavior,
            "status": "åˆ†æå®Œæˆ",
            "heatmap_images": heatmap_images
        }
        
    except Exception as e:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_file' in locals() and temp_file.exists():
            temp_file.unlink()
        
        # è¯¦ç»†çš„é”™è¯¯æ—¥å¿—è®°å½•
        import traceback
        error_details = {
            "error_type": type(e).__name__,
            "error_message": str(e),
            "traceback": traceback.format_exc()
        }
        print(f"è¡Œä¸ºçƒ­åŠ›åˆ†æé”™è¯¯è¯¦æƒ…: {error_details}")
        
        # å¦‚æœæ˜¯HTTPExceptionï¼Œç›´æ¥é‡æ–°æŠ›å‡ºä»¥ä¿æŒåŸå§‹é”™è¯¯ä¿¡æ¯
        if isinstance(e, HTTPException):
            raise e
        
        # å¯¹äºå…¶ä»–å¼‚å¸¸ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        error_message = str(e) if str(e) else f"æœªçŸ¥é”™è¯¯: {type(e).__name__}"
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {error_message}")

@app.post("/api/heatmap/overall")
async def overall_heatmap_analysis(
    file: UploadFile = File(...),
    stamp_min: Optional[float] = Form(None),
    stamp_max: Optional[float] = Form(None),
    sort_method: str = Form("peak"),
    calcium_wave_threshold: float = Form(1.5),
    min_prominence: float = Form(1.0),
    min_rise_rate: float = Form(0.1),
    max_fall_rate: float = Form(0.05)
):
    """æ•´ä½“çƒ­åŠ›å›¾åˆ†æ"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"overall_heatmap_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"æ•´ä½“çƒ­åŠ›å›¾åˆ†æï¼Œæ–‡ä»¶: {temp_file}")
        
        # è¯»å–æ•°æ®
        data = pd.read_excel(temp_file)
        print(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = OverallHeatmapConfig()
        config.STAMP_MIN = stamp_min
        config.STAMP_MAX = stamp_max
        config.SORT_METHOD = sort_method
        config.CALCIUM_WAVE_THRESHOLD = calcium_wave_threshold
        config.MIN_PROMINENCE = min_prominence
        config.MIN_RISE_RATE = min_rise_rate
        config.MAX_FALL_RATE = max_fall_rate
        
        # ç”Ÿæˆæ•´ä½“çƒ­åŠ›å›¾
        fig, info = generate_overall_heatmap(data, config)
        
        # å°†å›¾è¡¨è½¬æ¢ä¸ºbase64
        plot_base64 = save_plot_as_base64(fig)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()
        
        return {
            "success": True,
            "filename": file.filename,
            "heatmap_image": f"data:image/png;base64,{plot_base64}",
            "analysis_info": info,
            "config": {
                "stamp_min": stamp_min,
                "stamp_max": stamp_max,
                "sort_method": sort_method,
                "calcium_wave_threshold": calcium_wave_threshold,
                "min_prominence": min_prominence,
                "min_rise_rate": min_rise_rate,
                "max_fall_rate": max_fall_rate
            },
            "message": "æ•´ä½“çƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        print(f"æ•´ä½“çƒ­åŠ›å›¾åˆ†æé”™è¯¯: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_file' in locals() and temp_file.exists():
            temp_file.unlink()
        raise HTTPException(status_code=500, detail=f"æ•´ä½“çƒ­åŠ›å›¾åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/heatmap/em-sort")
async def em_sort_heatmap_analysis(
    file: UploadFile = File(...),
    stamp_min: Optional[float] = Form(None),
    stamp_max: Optional[float] = Form(None),
    sort_method: str = Form("peak"),
    custom_neuron_order: Optional[str] = Form(None),
    calcium_wave_threshold: float = Form(1.5),
    min_prominence: float = Form(1.0),
    min_rise_rate: float = Form(0.1),
    max_fall_rate: float = Form(0.05),
    sampling_rate: float = Form(4.8)
):
    """EMæ’åºçƒ­åŠ›å›¾åˆ†æ"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_file = TEMP_DIR / f"em_sort_heatmap_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
        with open(temp_file, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        print(f"EMæ’åºçƒ­åŠ›å›¾åˆ†æï¼Œæ–‡ä»¶: {temp_file}")
        
        # è¯»å–æ•°æ®
        data = pd.read_excel(temp_file)
        print(f"æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        
        # è§£æè‡ªå®šä¹‰ç¥ç»å…ƒé¡ºåº
        custom_order = None
        if custom_neuron_order:
            try:
                # å‡è®¾ä¼ å…¥çš„æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                custom_order = [neuron.strip() for neuron in custom_neuron_order.split(',') if neuron.strip()]
            except:
                custom_order = None
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = EMSortHeatmapConfig(
            stamp_min=stamp_min,
            stamp_max=stamp_max,
            sort_method=sort_method,
            custom_neuron_order=custom_order,
            calcium_wave_threshold=calcium_wave_threshold,
            min_prominence=min_prominence,
            min_rise_rate=min_rise_rate,
            max_fall_rate=max_fall_rate,
            sampling_rate=sampling_rate
        )
        
        # ç”ŸæˆEMæ’åºçƒ­åŠ›å›¾
        fig, info = analyze_em_sort_heatmap(data, config)
        
        # å°†å›¾è¡¨è½¬æ¢ä¸ºbase64
        plot_base64 = save_plot_as_base64(fig)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_file.unlink()
        
        return {
            "success": True,
            "filename": file.filename,
            "heatmap_image": f"data:image/png;base64,{plot_base64}",
            "analysis_info": info,
            "config": {
                "stamp_min": stamp_min,
                "stamp_max": stamp_max,
                "sort_method": sort_method,
                "custom_neuron_order": custom_neuron_order,
                "calcium_wave_threshold": calcium_wave_threshold,
                "min_prominence": min_prominence,
                "min_rise_rate": min_rise_rate,
                "max_fall_rate": max_fall_rate,
                "sampling_rate": sampling_rate
            },
            "message": "EMæ’åºçƒ­åŠ›å›¾ç”Ÿæˆå®Œæˆ"
        }
        
    except Exception as e:
        print(f"EMæ’åºçƒ­åŠ›å›¾åˆ†æé”™è¯¯: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_file' in locals() and temp_file.exists():
            temp_file.unlink()
        raise HTTPException(status_code=500, detail=f"EMæ’åºçƒ­åŠ›å›¾åˆ†æå¤±è´¥: {str(e)}")

@app.post("/api/heatmap/multi-day")
async def multi_day_heatmap_analysis(
    files: List[UploadFile] = File(...),
    day_labels: str = Form(...),  # é€—å·åˆ†éš”çš„å¤©æ•°æ ‡ç­¾ï¼Œå¦‚ "day0,day3,day6,day9"
    sort_method: str = Form("peak"),
    calcium_wave_threshold: float = Form(1.5),
    min_prominence: float = Form(1.0),
    min_rise_rate: float = Form(0.1),
    max_fall_rate: float = Form(0.05),
    create_combination: bool = Form(True),
    create_individual: bool = Form(True)
):
    """å¤šå¤©æ•°æ®ç»„åˆçƒ­åŠ›å›¾åˆ†æ"""
    try:
        # è§£æå¤©æ•°æ ‡ç­¾
        day_labels_list = [label.strip() for label in day_labels.split(',') if label.strip()]
        
        if len(files) != len(day_labels_list):
            raise HTTPException(
                status_code=400, 
                detail=f"æ–‡ä»¶æ•°é‡({len(files)})ä¸å¤©æ•°æ ‡ç­¾æ•°é‡({len(day_labels_list)})ä¸åŒ¹é…"
            )
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶å¹¶è¯»å–æ•°æ®
        data_dict = {}
        temp_files = []
        
        for i, (file, day_label) in enumerate(zip(files, day_labels_list)):
            temp_file = TEMP_DIR / f"multiday_{day_label}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{file.filename}"
            temp_files.append(temp_file)
            
            with open(temp_file, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # è¯»å–æ•°æ®
            data = pd.read_excel(temp_file)
            data_dict[day_label] = data
            print(f"{day_label}æ•°æ®åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {data.shape}")
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        config = MultiDayHeatmapConfig(
            sort_method=sort_method,
            calcium_wave_threshold=calcium_wave_threshold,
            min_prominence=min_prominence,
            min_rise_rate=min_rise_rate,
            max_fall_rate=max_fall_rate
        )
        
        # æ‰§è¡Œå¤šå¤©çƒ­åŠ›å›¾åˆ†æ
        results = analyze_multiday_heatmap(
            data_dict, 
            config, 
            correspondence_table=None,  # æš‚æ—¶ä¸æ”¯æŒå¯¹åº”è¡¨
            create_combination=create_combination,
            create_individual=create_individual
        )
        
        # è½¬æ¢å›¾å½¢ä¸ºbase64
        response_data = {
            "success": True,
            "filenames": [file.filename for file in files],
            "day_labels": day_labels_list,
            "analysis_info": results['analysis_info'],
            "config": {
                "sort_method": sort_method,
                "calcium_wave_threshold": calcium_wave_threshold,
                "min_prominence": min_prominence,
                "min_rise_rate": min_rise_rate,
                "max_fall_rate": max_fall_rate,
                "create_combination": create_combination,
                "create_individual": create_individual
            }
        }
        
        # æ·»åŠ ç»„åˆçƒ­åŠ›å›¾
        if results['combination_heatmap']:
            combo_base64 = save_plot_as_base64(results['combination_heatmap']['figure'])
            response_data['combination_heatmap'] = {
                "image": f"data:image/png;base64,{combo_base64}",
                "info": results['combination_heatmap']['info']
            }
        
        # æ·»åŠ å•ç‹¬çƒ­åŠ›å›¾
        individual_heatmaps = []
        for day, heatmap_data in results['individual_heatmaps'].items():
            individual_base64 = save_plot_as_base64(heatmap_data['figure'])
            individual_heatmaps.append({
                "day": day,
                "image": f"data:image/png;base64,{individual_base64}",
                "info": heatmap_data['info']
            })
        
        response_data['individual_heatmaps'] = individual_heatmaps
        response_data['message'] = f"å¤šå¤©çƒ­åŠ›å›¾åˆ†æå®Œæˆï¼Œå¤„ç†äº†{len(day_labels_list)}å¤©çš„æ•°æ®"
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in temp_files:
            if temp_file.exists():
                temp_file.unlink()
        
        return response_data
        
    except Exception as e:
        print(f"å¤šå¤©çƒ­åŠ›å›¾åˆ†æé”™è¯¯: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if 'temp_files' in locals():
            for temp_file in temp_files:
                if temp_file.exists():
                    temp_file.unlink()
        
        # å¦‚æœæ˜¯HTTPExceptionï¼Œç›´æ¥é‡æ–°æŠ›å‡º
        if isinstance(e, HTTPException):
            raise e
        
        raise HTTPException(status_code=500, detail=f"å¤šå¤©çƒ­åŠ›å›¾åˆ†æå¤±è´¥: {str(e)}")

@app.get("/api/download/{filename}")
async def download_file(filename: str):
    """ä¸‹è½½ç»“æœæ–‡ä»¶"""
    file_path = RESULTS_DIR / filename
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )

if __name__ == "__main__":
    import uvicorn
    import os
    
    # è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³HTTPå¤´éƒ¨å¤§å°é—®é¢˜
    os.environ['UVICORN_H11_MAX_INCOMPLETE_EVENT_SIZE'] = '65536'
    
    print("ğŸš€ å¯åŠ¨é’™ä¿¡å·åˆ†æå¹³å°åç«¯æœåŠ¡...")
    print("ğŸ“‹ æœåŠ¡é…ç½®:")
    print(f"   - ç›‘å¬åœ°å€: 0.0.0.0:8000")
    print(f"   - è¯·æ±‚å¤´å¤§å°é™åˆ¶: 65536 bytes (64KB)")
    print(f"   - æ–‡ä»¶ä¸Šä¼ é™åˆ¶: 200MB")
    print(f"   - å¹¶å‘è¿æ¥æ•°: 2000")
    print(f"   - è¶…æ—¶è®¾ç½®: 60ç§’")
    
    # å¯åŠ¨uvicornæœåŠ¡å™¨ï¼Œä½¿ç”¨ä¼˜åŒ–çš„é…ç½®è§£å†³431é”™è¯¯
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8000,
        # HTTPè¿æ¥é…ç½®
        limit_max_requests=2000,
        limit_concurrency=2000,
        timeout_keep_alive=60,
        timeout_graceful_shutdown=60,
        
        # å¢åŠ è¯·æ±‚å¤´å¤§å°é™åˆ¶åˆ°64KBï¼ˆè§£å†³431é”™è¯¯ï¼‰
        h11_max_incomplete_event_size=65536,
        
        # å·¥ä½œè¿›ç¨‹é…ç½®
        workers=1,
        
        # æ—¥å¿—é…ç½®
        log_level="info",
        access_log=True,
        
        # é‡æ–°åŠ è½½é…ç½®ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
        reload=False,  # è®¾ä¸ºFalseé¿å…å¼€å‘æ—¶çš„é‡è½½é—®é¢˜
        
        # SSLé…ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        ssl_keyfile=None,
        ssl_certfile=None,
        
        # å…¶ä»–ä¼˜åŒ–é€‰é¡¹
        loop="auto",
        lifespan="on",
    )